\addcontentsline{toc}{part}{Part I}
\chapter{\label{cha:intro}Introduction}

Compilers are ubiquitous and pivotal components
at the core of innumerable software and programming language ecosystems.
They translate the high-level, human-readable source code
of computer programs into target-specific instructions that machines
can parse and execute.
As a consequence, the ramifications that arise from flawed
compiler implementations can be both far-reaching and severe.
The outstanding complexity and feature richness of modern compilers has created
several formidable challenges, the scope of which is perhaps best exemplified
by \citet{hoare2003verifying}
declaring the task of building a provably correct compiler to be a 
\textit{Grand Challenge} in computing research.
In recent decades, researchers and practitioners
have invested tremendous resources
into compiler research, attempting to improve their
performance, quality, and reliability. 
Such efforts are especially beneficial for newer software systems,
that require robust compilation pipelines to establish themselves
as worthy alternatives to mature standards.


\section{Compiler Testing}

Because of their cornerstone role in many critical applications,
compilers are often subject to thorough quality assurance processes.
In practice, compiler developers often rely on testing as a means
of assessing and improving the correctness of compilers.
Empirical evidence supports the benefits of such approaches,
with \citet{sun2016toward} and \citet{holler2012fuzzing} highlighting
the existence of bugs in widely used C compilers and JavaScript engines, respectively. 
These findings stand out because of the maturity of the
software ecosystems that pivot around these technologies, showcasing
the necessity of effective testing methods.

One approach to testing compilers is to assemble manually-written test suites.
This strategy has several advantages, including the possibility to individually
target specific components of the compiler and leveraging developers' capacity to
reason about the tests' semantics and expected outputs.
However, the labor-intensive nature of this process and 
the imposing complexity of compiler code bases have
led to automated test case generation methods superseding their 
manual counterparts \citep{chen2020survey, zhao2009automated}.
Though compelling and time-effective, automated test case
generation strategies concurrently give rise to an arduous set of challenges.
The infinite number of correct possible programs,
the syntactic dependencies between code snippets,
% the broad range of compiler features,
and the many semantic nuances programming languages exhibit all constitute
obstacles that effective test generation tools must address.

Fuzzing, or random testing, lends itself naturally to this task thanks to its
ability to generate code snippets that trigger diverse behavior within the compiler.
Because of this, researchers have proposed a myriad of language-specific approaches, 
fixated around two overarching paradigms.
Algorithms that generate standalone code such as those proposed by  
\citet{yang2011finding, holler2012fuzzing, veggalam2016ifuzzer}, and \citet{havrikov2019systematically}
have the ability to generate entirely novel test programs, which in turn may contain
seldom encountered constructs and relations.
By contrast, mutation-centric approaches such as those put forward by 
\citet{le2014compiler, le2015finding, sun2016finding}, and \citet{stepanov2021type}
rely on an input (or seed) program, that serves as a basis for subsequent transformations.
While the former approach may be able to explore more of the
space of feasible input programs, the latter effectively leverages
large corpora that become available as programming language
ecosystems mature.

\section{The Kotlin Programming Language}

Kotlin \footnote{https://kotlinlang.org/} is a relatively new programming language whose development
initiative is led by JetBrains.
Initially designed as a Java alternative, the Kotlin
ecosystem has steadily grown and currently envelops
a broad range of environments, including server-side development,
web frontend interfaces, and mobile applications. 
Kotlin showcases compelling features, including expressiveness,
null safety, and Java interoperability, which played a key role in Google embracing
a "Kotlin-first" approach in its Android mobile operating system \cite{kotlinfirst}.

The number of research initiatives investigating Kotlin's impact 
in the software development process has risen in tandem with its adoption.
Several studies have analyzed both Kotlin's influence
on the quality of code bases, as well as developers' perception
of Kotlin in comparison to their previous experience.
\citet{flauzino2018you}  and \citet{gois2019empirical} independently
found that code bases which at least partially employ Kotlin tend to
exhibit fewer code smells than their Java counterparts. 
\citet{ardito2020effectiveness} suggest that transitioning projects
from Java to Kotlin can meaningfully increase code conciseness.
\citet{chauhan2021performance} provide empirical evidence indicating
that Kotlin's coroutine implementation vastly outperforms
Java's thread-centric concurrency framework in terms of speed.
\citet{oliveira2020adoption} study Android practitioners' opinions regarding
Kotlin adoption and reveal that developers find Kotlin easy
to adopt and understand. 
Their study also showcases that developers value Kotlin's
null-safety guarantees, as well as its interoperability with Java.

Despite the sharp rise in Kotlin's popularity and an increasing
number of systems relying on its ecosystem, the study by \citet{stepanov2021type}
is the only one to propose a tailored algorithm for automatically 
testing the Kotlin compiler.
Their work heavily relies on manually written test suites to 
provide seeds for an enumeration-based fuzzer aimed at detecting compilation
errors between different compiler implementations. 
Currently, no tool that is capable of generating entirely novel, semantically 
meaningful, and diverse test programs for the Kotlin compiler exists.
Creating such an tool would not only aid the quality assurance process
of the Kotlin compiler, but also provide valuable insight 
into the relation between language features and compiler defects.
% fuzzing strategies are best suited for revealing compiler bugs.

\section{\label{sec:aim}Research Aim and Scope}

This thesis seeks to advance the current understanding of the
effectiveness of test program-based fuzzing as a means of automated compiler testing.
To narrow the scope of this goal, we identify and acknowledge several
key limitations.
First, this thesis only concerns itself with empirical analyses regarding
the Kotlin compiler. 
Though the prototype implementation may well be 
extended to target other systems, we restrain the
evaluation to Kotlin and its current compiler versions ensure feasibility.

Second, this study investigates the ability of the proposed fuzzing
techniques to find bugs in the targeted compiler, and does not emphasize
the practical or commercial implications of the uncovered bugs.
We provide a qualitative analysis of the uncovered defects, but do
not further investigate the ramifications of their existence.
Moreover, research concerning other widely studied defect-centric 
practices such as bug localization, program minimization, and bug deduplication
are outside the scope of this study.
Within these constraints, we seek to achieve the following research aim:

\begin{quote}
\centering 
\emph{The aim of this research is to gain insight into how 
effective heuristic-driven 
test program-based fuzzing is at uncovering bugs in the
Kotlin compiler.}
\end{quote}

To address this aim, we implement a grammar-aided fuzzing tool
that generates varied and valid pieces of Kotlin code. 
The generated code serves as input to the compiler,
whose behavior is in turn assessed through differential testing.
We divide the overarching research aim into several, more granular
research questions.

A vital point to consider when designing a fuzzing algorithm is
its potential to adapt to the goals of its users. \citet{amalfitano2015exploiting}
show that random testing approaches eventually reach a
saturation point, where novel input is exceedingly unlikely to trigger new faults. 
To combat this, practitioners can either design new tools, adapt existing
ones, or intertwine approaches.
While flexible design can help mitigate this shortcoming, studying the 
impact of cornerstone hyperparameters and heuristic choices
is crucial for understanding the scope of a fuzzer and its behavior.
To this end, we formulate the following research question:

\begin{quote}
\centering 
\emph{\textbf{RQ1:} How do meta-heuristic guidance
criteria influence the properties of test programs
generated by fuzzing?}
\end{quote}

Next, we focus the study on comparing the performance of
several established and novel search formulations.
The second research question aims to investigate
the relative bug-uncovering potential of such heuristics
within an evolutionary framework:

\begin{quote}
\centering 
\emph{\textbf{RQ2:} How do the proposed generative heuristics
compare in terms of uncovering bugs in the Kotlin compiler?}
\end{quote}

% The conjoined analysis of the research questions unlocks both
% an empirical estimation of the raw performance of the proposed
% approach, as well as a deeper understanding of the underlying
% mechanics of the algorithms and their ramifications.

\section{Contributions and Significance}

This thesis makes both theoretical and practical advances,
relevant to both the field of compiler testing and to the 
Kotlin community.
The study makes the four following contributions:

\begin{enumerate}
	\item Three novel formulations of grammar-aided test program-based compiler fuzzing,
including a random sampling approach
and two evolutionary algorithms revolving
around diversity- and proximity-driven criteria.
	\item A modular, configurable, and extendable framework for
end-to-end automated fuzzing of the Kotlin compiler,
implementing the three proposed approaches.
	\item Empirical analyses comparing the bug finding capabilities
and heuristic-driven behavior of the three novel fuzzing algorithms.
	\item A replication package containing open-source artifacts, and analytics tools
for validating, verifying, and extending the current study.
\end{enumerate}

From a theoretical perspective, we investigate the applicability
and adaptation of evolutionary fuzzing techniques to the task of compiler testing. 
The significance of this knowledge is tied to the continued
interest in establishing reliable quality assurance measures for compilers.
On a practical level, we provide
prototype implementation that supports the proposed algorithms
and includes additional infrastructure surrounding the core fuzzing process.
We aim for this tool to become a useful instrument for improving the 
reliability and quality of the Kotlin compiler.
Compiler engineers can leverage such a tool as an additional
step in a continuous integration pipeline, or choose to employ it
when developing novel features in future versions of the language.

\section{Overview of the Study}

This thesis consists of eight additional chapters split between in three main parts.
Part I consists of Chapters \ref{cha:background} and \ref{cha:related}, and
provides an overview of the relevant body of knowledge and literature.
Chapter \ref{cha:background} introduces the theoretical background
that constitutes the foundation of this study.
Chapter \ref{cha:related} discusses related work and positions this
thesis within the current research landscape.

Part II encompasses Chapters \ref{cha:algorithm}, through \ref{cha:results}
and details the main contributions of this thesis. 
Chapter \ref{cha:algorithm} describes the conceptual outline of
the fuzzing procedure and the rationale behind its
underpinning design choices.
Chapter \ref{cha:tool} delineates the prototypical implementation
of our tool and highlights its practical applications.
Chapter \ref{cha:study} outlines the design of the empirical
study carried out to evaluate the performance of our tool.
Chapter \ref{cha:results} analyzes the results of the empirical study
and ties them to the main research questions.

Part III includes Chapters \ref{cha:discussion} and \ref{cha:conc} 
and reflects upon the outcomes of this study.
Chapter \ref{cha:discussion} discusses the main 
findings of this research, its limitations, and makes additional recommendations for future research.
Finally, Chapter \ref{cha:conc} concludes the thesis.

