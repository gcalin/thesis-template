\chapter{\label{cha:study}Empirical Study}

This chapter outlines the empirical evaluation we carried out
to evaluate the prototype implementation of \kf~ its six
constituent search algorithms.
\Cref{sec:subjects} begins by establishing the algorithms
we set out to assess within this study, and \Cref{sec:rqs} fixates the research
questions we aim to answer.
\Cref{sec:metrics} establishes the metrics we use to determine the
performance and behavior of each algorithm.
Lastly, \Cref{sec:protocol} details the experimental protocol
we follow to obtain concrete results.

\section{\label{sec:subjects}Evaluation Subjects}

\newacronym{SODGA}{SODGA}{Single-Objective Diversity Genetic Algorithm}
\newacronym{MODGA}{MODGA}{Many-Objective Diversity Genetic Algorithm}
\newacronym{STPGA}{STPGA}{Single-Target Proximity Genetic Algorithm}
\newacronym{MOPGA}{MOPGA}{Many-Objective Proximity Genetic Algorithm}
\newacronym{WSPGA}{WSPGA}{Whole-Suite Proximity Genetic Algorithm}

To gather a comprehensive overview of the performance, behavior,
and implications of the approaches proposed in \Cref{cha:algorithm},
we evaluate six prototype implementations, split between three distinct categories.
The first category contains a single algorithm, \gls{RS},
introduced in \Cref{subsec:random-sampling}.
The second class contains two instances of syntactic-oriented
algorithms: \gls{SODGA} and \gls{MODGA}, based on the formulations
proposed in \Cref{subsec:diversity-ga}.
The final group of algorithms focuses on the semantic proximity
formulations detailed in \Cref{subsec:proximity-ga}, and contains three instances.
\gls{STPGA}, \gls{MOPGA}, and \gls{WSPGA}
implement the three proposed approaches.

\Cref{tab:algs} contains an overview of the six implemented algorithms.
\gls{SODGA}, \gls{STPGA}, \gls{WSPGA} behave like \gls{SO} algorithms
(though \gls{STPGA} repeatedly switches objectives),
while \gls{MODGA} and \gls{MOPGA} implement \gls{MO} formulations.
We opt for the use of tournament selection for \gls{SO} scenarios, and
domination rank counterparts for \gls{MO} instances.
For \gls{GA}-based algorithms, we turn to established literature values
to determine the size of the population and further selection details.
However, applications of evolutionary fuzzing to
compiler testing are scarce, and established evolutionary fuzzing
tools like \textsc{VUzzer} \cite{rawat2017vuzzer} and
\textsc{V-Fuzz} \cite{li2019v} make no recommendation in these regards.

As such, we turn to standard values used in software
testing literature, particularly of \textsc{EvoSuite}, which uses
a population size of 50 and a tournament size of 10
\cite{fraser2011evosuite, panichella2017automated}.
Past research by \citet{arcuri2013parameter}
has shown that default values can provide solid
performance in a broad set of scenarios, in the context of software testing.
Though these findings provide no guarantees for the task of compiler
testing in particular, we believe these are sensible
starting points that circumvent the expensive requirements of parameter
tuning over all proposed algorithms.

\begin{table}[]
    \centering
    \begin{tabular}{cccc}
    % \toprule
    Heuristic & Fitness & \# Objectives & Selection \\
    \midrule
    \gls{RS} & - & - & - \\
    \midrule
    \gls{SODGA} & $f^{(SO)}_{\texttt{DIV}}(b, P) = \frac{1}{1 + dis(b, P)}$
    & 1 & Tournament\\
    \midrule
    \gls{MODGA} & $f^{(MO)}_{\texttt{DIV}}(b) = \left\lbrace -f_{sz}(b), ~f_{l_i}(b)|~l_i \in \mathbb{L}
    \right\rbrace$ & $\mid \mathbb{L} \mid + 1 = 6$ & Dom. Rank, Tournament\\
    \midrule
    \gls{STPGA} & $f^{(SO)}_{\texttt{PRO}}(b, t_i) = d(e(b), t_i)$ & 1, adaptive & Tournament\\
    \midrule
    \gls{MOPGA} & $f^{(MO)}_{\texttt{PRO}}(b, \mathcal{T}) =
    \left\lbrace d(e(b), t_i)|~t_i \in \mathcal{T} \right\rbrace$ & $\mid \mathcal{T} \mid$ & Dom. Rank, Tournament\\
    \midrule
    \gls{WSPGA} & $f^{(SO)}_{\texttt{WS}}(S, \mathcal{T}) =
    \sum_{t \in \mathcal{T}} \min_{b^{(i)} \in S} d(e(b^{(i)}), t)$ & 1 & Suite Tournament\\
    \bottomrule
    \end{tabular}
    \caption{Overview of evaluated algorithms.}
    \label{tab:algs}
\end{table}

\section{\label{sec:rqs}Research Questions}

The empirical analysis of the proposed approach aims to answer the following
research questions, which we reiterate from \Cref{sec:aim}:

\begin{quote}
\centering 
\emph{\textbf{RQ1:} How do meta-heuristic guidance
criteria influence the properties of test programs
generated by fuzzing?}
\end{quote}

To better differentiate between the six proposed approaches and their
individual most significant guiding parameters, we split the \textit{RQ1}
into three subquestions:

\begin{quote}
\centering 
\emph{\textbf{RQ1.1:} How does the simplicity bias
impact the properties of files generated by \gls{RS}?}
\end{quote}

\begin{quote}
\centering 
\emph{\textbf{RQ1.2:} How does the projection space
topology influence the properties of files generated
by syntactic diversity-driven heuristics?}
\end{quote}

\begin{quote}
\centering 
\emph{\textbf{RQ1.3:} How does target selection
influence the properties of files generated
by semantic proximity-driven heuristics?}
\end{quote}

The three subquestions address each of three generative
heuristic categories by tuning their core elements.
For \gls{RS}, by far the most influential hyperaparameter is
the simplicity bias, which governs both the structure and complexity of generate expressions.
For diversity heuristics, the manner in which diversity is computed drives the selection procedure.
Finally, the number of targets that proximity-heuristics receive is crucial for their search process.
Once established, the most sensible settings revealed by the previous subquestions can be used
to empirically assess the performance of each category of algorithms in comparison to the others,
as stated in \textit{RQ2}:

\begin{quote}
\centering 
\emph{\textbf{RQ2:} How do the random search, syntactic
diversity, and semantic proximity generative heuristics
perform in terms of uncovering bugs in the Kotlin compiler?}
\end{quote}

To answer the second research question, we seek to measure
the performance of the best algorithm from each class and compare
both their bug-finding capabilities and efficiency.

\section{\label{sec:metrics}Evaluation Metrics}

To measure the performance of our algorithms, we focus on the two
dimensions of \textit{effectiveness} and \textit{efficiency}.
Before addressing those measures, however, we first establish
the meaning of a compiler defect within the framework
of \gls{DT}.
For the purposes of this study, a generated piece of Kotlin code
$b$ passes through two versions of the compiler,
\texttt{K1} and \texttt{K2}.
$b$ is said to have uncovered a deffect if either of following cases occurs:

\begin{enumerate}
    \item \texttt{K1} successfully compiles $b$ and \texttt{K2} does not.
    \item \texttt{K1} compiles $b$ and \texttt{K2} crashes.
    \item \texttt{K2} successfully compiles $b$ and \texttt{K1} does not.
    \item \texttt{K2} compiles $b$ and \texttt{K1} crashes.
    \item \texttt{K1} and \texttt{K2} both crash.
\end{enumerate}

In the above listing, we define a \textit{successful} compilation
as one that results in an output object
(for our purposes, a \texttt{jar} file).
An unsuccessful compilation is one that \textit{completes}, but
does not result in an output object (i.e., the compiler
has detected that the code is erroneous).
A \textit{crash} occurs when compilation fails to complete altogether.
\Cref{tab:defects} contains a visualization of the types of defects
that can emerge during a differential testing round.

Only two types of behaviors are considered considered correct within
this system: either both systems successfully compile an input
file to a \texttt{jar} object, or both fail to do so, without crashing.
In any other case, we consider the behavior erroneous and attach it a 
label that categorizes the defect according to the list above.
Since the tangible goal of this study is to help improve
the quality of the \texttt{K2} compiler, we only focus on error types
that break backwards compatibility (I and III), or that cause \texttt{K2}
to crash (IV and V).
We emphasize errors that only cause defects in \texttt{K1} (type II)
less than their counterparts, as this version is to be replaced in the future.

\begin{table}[]
    \centering
    \begin{tabular}{cc|ccc}
        &&\multicolumn{3}{c}{\texttt{K1} outcome}\\
        &&Success & Failure & Crash\\
        \midrule
        \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\texttt{K2} outcome}}}& Success & \ding{52} & Type I & Type II \\
        & Failure & Type III & \ding{52} & Type II\\
        & Crash & Type IV & Type IV & Type V\\
    \end{tabular}
    \caption{Types of compiler defects.}
    \label{tab:defects}
\end{table}

\newacronym{AUC}{AUC}{Area-Under-Curve}

We use this notion of defects to measure the
performance of the six proposed algorithms.
The effectiveness of a fuzzer run is the number
of files generated within that run that cause any defects (types I-V)
after differential testing.
This metric captures the performance of the fuzzer after the exhaustion,
however, an additional measure of efficiency is required to underline
an algorithm's behavior over time.
To this end, we express the efficiency of a run through the
\gls{AUC} metric.

Informally, when running a fuzzer algorithm, snapshots of its performance
can be expressed as 2-dimensional points, where the $x$ axis represents the time elapsed,
and the $y$ axis, the number of uncovered defects.
After several snapshots have been taken at approximately comparable
times for multiple algorithms, the curve that emerges from connecting the points
corresponding to snapshots splits the space in two.
The area under this curve provides a means of comparing how quickly
algorithms uncover bugs.
Given a set $P$ of two dimensional points $p_i$, with $p_i^{(1)}$
the time at which the snapshot was taken and $p_i^{(2)}$ the number of defects
uncovered by generated files until that point,
we compute the \gls{AUC} according to the trapezoidal rule approximation
given by \Cref{eq:auc}.

\begin{equation}
    \frac{\sum_{i=1}^{\mid P \mid - 1} \Large( p_i^{(2)} + p_{i + 1}^{(2)} \Large)
\cdot (p_{i + 1}^{(1)} - p_{i}^{(1)})}{p_{\mid P \mid}^{(1)} - p_1^{(1)}}
\label{eq:auc}
\end{equation}

\section{\label{sec:protocol}Experimental Protocol}

To ensure the robustness of the empirical evaluation, we perform repeated runs
to determine the relative performance of the six proposed methods.
Due to the large number of possible hyperparameter and heuristic
combinations, as well as the large quantity of computational resources
required to perform \gls{DT} on generated files, we focus these
techniques on performance evaluations.
To determine appropriate parameter ranges for variables not explored in previous work,
we rely on fewer, longer runs, instead.

\subsubsection{Statistical Analysis}

Where appropriate, we additionally perform statistical tests in accordance
with standard practices related to empirical analysis of
randomized software engineering algorithms \cite{arcuri2014hitchhiker}.
We employ the paired-data version of the
Wilcoxon \cite{conover1999practical} signed-rank test to verify
whether the performance (either efficiency or effectiveness) of one algorithm
differs from that of another.
This test operates on a null hypothesis that the distribution
of the difference between two sample populations is symmetric around a mean of 0.
Informally, the main consequence of the alternative hypothesis is that
there is a significant difference between the measurements emerging from the 
two algorithms subject to comparison.
The $p$-value computed by this test determines whether the evidence
supports the null or the alternative hypothesis.
\textit{Significant} $p$-values ($<$ 0.05) imply a high probability of the latter being true.

To measure the how much the performance of two
algorithms differs in favor of either one, we use the method
established by \citet{vargha2000critique} $A_{12}$ for computing the \textit{effect size}
of a population of samples.
The $A_{12}$ captures the stochastic difference between the 
distributions approximated by the two populations and, with respect
to our study, shows which distribution skews higher based on evidence from
several runs.
An $A_{12}$ measurement of $0.5$ in a comparison between two 
algorithms $a_1$ and $a_2$ denotes the two approaches
are equivalent.
Higher values favor $a_2$, while, symmetrically, lower values favor $a_1$.

\subsubsection{Experiment Length and Heuristic}

To understand how the simplicity bias influences the nature
of generated files (\textit{RQ1.1}), we test several values of this parameter under
the setting of \gls{RS}.
Random Sampling is a prime candidate for uncovering the effects of this
option, as there are very few external factors that could otherwise alter
the shape of block generation.
We experiment simplicity bias values of between 0.4 and 0.6, as we empirically found
that values outside this range either lead to blocks too large to scale in
more sophisticated search heuristics, or too small to build a sufficiently
complex context.
We run \kf~ using \gls{RS} for 90 minutes for each setting of simplicity bias.

To answer \textit{RQ1.2}, we perform two 90-minute runs for both \gls{SODGA} and \gls{MODGA}:
one with a Euclidean norm distance measure ($d(e(b_1), e(b_2)) =
\sum_{1 \leq i \leq \mid e(b_{\{1,2\}}) \mid} \sqrt{(b_1^{(i)} - b_2^{(i)}}$),
and one with an $l^\infty$ norm equivalent ($d(e(b_1), e(b_2)) =
\max_{1 \leq i \leq \mid e(b_{\{1,2\}}) \mid} \mid b_1^{(i)} - b_2^{(i)} \mid$.
The results would help uncover how a more stringent measures of diversity
influences the quality of the generated files.
Next, we attempt to answer \textit{RQ1.3} by varying the number of targets provided
to \gls{STPGA}, \gls{MOPGA}, and \gls{WSPGA} between 50 and 200, which
helps understand how the number of objectives affects
the generative process.
Finally, we answer \textit{RQ2} by considering the effectiveness and efficiency
of \gls{RS}, and the two most sensible approaches that emerge from the analysis of the
previous subquestions.

\subsubsection{Experimental Environment}

We carry out all \gls{DT} procedures through the publicly released
Kotlin version \texttt{1.8.20-RC-release-288}
\footnote{https://github.com/JetBrains/kotlin/releases/tag/v1.8.20}.
We ran all generation, \gls{DT}, and analysis experiments on a
AMD Ryzen 7 5800H machine, running on the Manjaro Talos 22.1.2
\gls{OS}, with 16 gigabytes of RAM.
We executed all experiments in isolated containers using Docker.

We ran 5 instances of \gls{RS} to answer \textit{RQ1.1}.
Three runs \gls{SODGA} and \gls{MODGA} provide the data for \textit{RQ1.2},
while four runs for each of the three proximity algorithms constitute the experimental
analysis for \textit{RQ1.3}.
We additionally executed 10 runs each for each of the most promising algorithm
from each class to answer \textit{RQ2},
for a total of $5 + 3 + 12 + 3 \times 10 = 50$ total fuzzing sessions.
We additionally perform one other long-term (82 hour) fuzzing campaign to
gain insight into the scale-related behavior and limitations of the fuzzer.